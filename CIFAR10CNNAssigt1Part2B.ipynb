{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN model for CIFAR-10\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 5\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "def createmodel1(epochs,lrate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    epochs = epochs\n",
    "    lrate = lrate\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createmodel1(25,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 130s - loss: 1.7034 - acc: 0.3849 - val_loss: 1.3682 - val_acc: 0.5072\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 124s - loss: 1.3428 - acc: 0.5188 - val_loss: 1.2410 - val_acc: 0.5630\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 122s - loss: 1.1774 - acc: 0.5802 - val_loss: 1.1063 - val_acc: 0.6001\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 122s - loss: 1.0505 - acc: 0.6244 - val_loss: 1.0404 - val_acc: 0.6306\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.9473 - acc: 0.6649 - val_loss: 0.9890 - val_acc: 0.6498\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.8645 - acc: 0.6933 - val_loss: 0.9623 - val_acc: 0.6617\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 121s - loss: 0.7880 - acc: 0.7208 - val_loss: 0.9378 - val_acc: 0.6733\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.7237 - acc: 0.7428 - val_loss: 0.9107 - val_acc: 0.6859\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 130s - loss: 0.6567 - acc: 0.7685 - val_loss: 0.9221 - val_acc: 0.6851\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 126s - loss: 0.6042 - acc: 0.7861 - val_loss: 0.9188 - val_acc: 0.6882\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 124s - loss: 0.5576 - acc: 0.8036 - val_loss: 0.9227 - val_acc: 0.6964\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.5144 - acc: 0.8179 - val_loss: 0.9182 - val_acc: 0.6984\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 124s - loss: 0.4766 - acc: 0.8323 - val_loss: 0.9381 - val_acc: 0.6965\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.4466 - acc: 0.8433 - val_loss: 0.9253 - val_acc: 0.7018\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.4111 - acc: 0.8535 - val_loss: 0.9296 - val_acc: 0.7048\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.3827 - acc: 0.8660 - val_loss: 0.9582 - val_acc: 0.7020\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 121s - loss: 0.3575 - acc: 0.8753 - val_loss: 0.9750 - val_acc: 0.7071\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.3369 - acc: 0.8809 - val_loss: 0.9701 - val_acc: 0.7073\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.3147 - acc: 0.8897 - val_loss: 0.9978 - val_acc: 0.7102\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.2992 - acc: 0.8951 - val_loss: 1.0090 - val_acc: 0.7053\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.2819 - acc: 0.9023 - val_loss: 1.0016 - val_acc: 0.7085\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.2627 - acc: 0.9081 - val_loss: 1.0171 - val_acc: 0.7102\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.2503 - acc: 0.9135 - val_loss: 1.0478 - val_acc: 0.7105\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 122s - loss: 0.2403 - acc: 0.9174 - val_loss: 1.0656 - val_acc: 0.7100\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 123s - loss: 0.2264 - acc: 0.9213 - val_loss: 1.0567 - val_acc: 0.7110\n",
      "Accuracy: 71.10%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep version of the simple CNN as shownabove.Introducing an additional round of convolutions with many more feature maps.\n",
    "#Using the same pattern of Convolutional, Dropout, Convolutional and Max Pooling layers.# Create the model\n",
    "def createmodel2(epochs,lrate,dropout,batch_size):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    epochs = 25\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model2.summary())\n",
    "    numpy.random.seed(seed)\n",
    "    model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
    "    # Final evaluation of the model\n",
    "    scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 16, 16)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 8, 8)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 8, 8)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 128, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,915,114\n",
      "Trainable params: 2,915,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 207s - loss: 1.9196 - acc: 0.2887 - val_loss: 1.6069 - val_acc: 0.4289\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 200s - loss: 1.4953 - acc: 0.4562 - val_loss: 1.3445 - val_acc: 0.5083\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 291s - loss: 1.2987 - acc: 0.5319 - val_loss: 1.1513 - val_acc: 0.5916\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 340s - loss: 1.1426 - acc: 0.5921 - val_loss: 1.0653 - val_acc: 0.6186\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 338s - loss: 1.0275 - acc: 0.6341 - val_loss: 0.9572 - val_acc: 0.6609\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 340s - loss: 0.9429 - acc: 0.6639 - val_loss: 0.9030 - val_acc: 0.6835\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 307s - loss: 0.8724 - acc: 0.6917 - val_loss: 0.8666 - val_acc: 0.6911\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 223s - loss: 0.8161 - acc: 0.7107 - val_loss: 0.8017 - val_acc: 0.7167\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 340s - loss: 0.7691 - acc: 0.7289 - val_loss: 0.8095 - val_acc: 0.7117\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 342s - loss: 0.7240 - acc: 0.7431 - val_loss: 0.7664 - val_acc: 0.7338\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 344s - loss: 0.6899 - acc: 0.7550 - val_loss: 0.7400 - val_acc: 0.7427\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 344s - loss: 0.6588 - acc: 0.7681 - val_loss: 0.7068 - val_acc: 0.7556\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 223s - loss: 0.6269 - acc: 0.7776 - val_loss: 0.7020 - val_acc: 0.7549\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.5973 - acc: 0.7886 - val_loss: 0.6734 - val_acc: 0.7638\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 203s - loss: 0.5790 - acc: 0.7951 - val_loss: 0.6652 - val_acc: 0.7666\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.5547 - acc: 0.8047 - val_loss: 0.6578 - val_acc: 0.7745\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.5314 - acc: 0.8110 - val_loss: 0.6406 - val_acc: 0.7793\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.5142 - acc: 0.8174 - val_loss: 0.6366 - val_acc: 0.7813\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.4926 - acc: 0.8260 - val_loss: 0.6469 - val_acc: 0.7819\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.4780 - acc: 0.8292 - val_loss: 0.6328 - val_acc: 0.7843\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.4631 - acc: 0.8356 - val_loss: 0.6365 - val_acc: 0.7858\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.4454 - acc: 0.8420 - val_loss: 0.6115 - val_acc: 0.7954\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.4326 - acc: 0.8479 - val_loss: 0.6175 - val_acc: 0.7939\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.4192 - acc: 0.8490 - val_loss: 0.6228 - val_acc: 0.7926\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.4053 - acc: 0.8553 - val_loss: 0.6168 - val_acc: 0.7958\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d4d4aaf1f201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreatemodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-82491863f8c7>\u001b[0m in \u001b[0;36mcreatemodel2\u001b[0;34m(epochs, lrate, dropout, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "createmodel2(25,0.01,0.2,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 3, 32, 32)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/22\n",
      "1562/1562 [==============================] - 172s - loss: 1.8168 - acc: 0.3402 - val_loss: 1.5195 - val_acc: 0.4454\n",
      "Epoch 2/22\n",
      "1562/1562 [==============================] - 171s - loss: 1.5185 - acc: 0.4507 - val_loss: 1.3734 - val_acc: 0.5086\n",
      "Epoch 3/22\n",
      "1562/1562 [==============================] - 171s - loss: 1.3991 - acc: 0.5001 - val_loss: 1.2145 - val_acc: 0.5732\n",
      "Epoch 4/22\n",
      "1562/1562 [==============================] - 172s - loss: 1.3228 - acc: 0.5306 - val_loss: 1.1550 - val_acc: 0.5971\n",
      "Epoch 5/22\n",
      "1562/1562 [==============================] - 171s - loss: 1.2496 - acc: 0.5601 - val_loss: 1.0750 - val_acc: 0.6298\n",
      "Epoch 6/22\n",
      "1562/1562 [==============================] - 171s - loss: 1.1941 - acc: 0.5819 - val_loss: 1.0455 - val_acc: 0.6325\n",
      "Epoch 7/22\n",
      "1562/1562 [==============================] - 171s - loss: 1.1419 - acc: 0.6005 - val_loss: 0.9946 - val_acc: 0.6549\n",
      "Epoch 8/22\n",
      "1562/1562 [==============================] - 170s - loss: 1.1072 - acc: 0.6124 - val_loss: 0.9416 - val_acc: 0.6715\n",
      "Epoch 9/22\n",
      "1562/1562 [==============================] - 172s - loss: 1.0713 - acc: 0.6252 - val_loss: 0.9590 - val_acc: 0.6684\n",
      "Epoch 10/22\n",
      "1562/1562 [==============================] - 171s - loss: 1.0425 - acc: 0.6359 - val_loss: 0.8905 - val_acc: 0.6958\n",
      "Epoch 11/22\n",
      "1562/1562 [==============================] - 171s - loss: 1.0221 - acc: 0.6442 - val_loss: 0.8818 - val_acc: 0.6982\n",
      "Epoch 12/22\n",
      "1562/1562 [==============================] - 174s - loss: 0.9986 - acc: 0.6511 - val_loss: 0.8652 - val_acc: 0.7017\n",
      "Epoch 13/22\n",
      "1562/1562 [==============================] - 170s - loss: 0.9772 - acc: 0.6603 - val_loss: 0.8491 - val_acc: 0.7025\n",
      "Epoch 14/22\n",
      "1562/1562 [==============================] - 169s - loss: 0.9638 - acc: 0.6648 - val_loss: 0.8117 - val_acc: 0.7216\n",
      "Epoch 15/22\n",
      "1562/1562 [==============================] - 170s - loss: 0.9476 - acc: 0.6711 - val_loss: 0.8237 - val_acc: 0.7180\n",
      "Epoch 16/22\n",
      "1562/1562 [==============================] - 171s - loss: 0.9274 - acc: 0.6766 - val_loss: 0.8281 - val_acc: 0.7189\n",
      "Epoch 17/22\n",
      "1562/1562 [==============================] - 172s - loss: 0.9128 - acc: 0.6851 - val_loss: 0.7938 - val_acc: 0.7339\n",
      "Epoch 18/22\n",
      "1562/1562 [==============================] - 170s - loss: 0.9057 - acc: 0.6864 - val_loss: 0.7637 - val_acc: 0.7388\n",
      "Epoch 19/22\n",
      "1562/1562 [==============================] - 169s - loss: 0.8871 - acc: 0.6919 - val_loss: 0.7747 - val_acc: 0.7329\n",
      "Epoch 20/22\n",
      "1562/1562 [==============================] - 171s - loss: 0.8825 - acc: 0.6959 - val_loss: 0.7477 - val_acc: 0.7450\n",
      "Epoch 21/22\n",
      "1562/1562 [==============================] - 170s - loss: 0.8718 - acc: 0.6985 - val_loss: 0.7386 - val_acc: 0.7450\n",
      "Epoch 22/22\n",
      "1562/1562 [==============================] - 170s - loss: 0.8601 - acc: 0.7032 - val_loss: 0.7264 - val_acc: 0.7570\n",
      "Model Accuracy = 0.73\n",
      "Actual Label = cat vs. Predicted Label = deer\n",
      "Actual Label = ship vs. Predicted Label = cat\n",
      "Actual Label = ship vs. Predicted Label = truck\n",
      "Actual Label = airplane vs. Predicted Label = cat\n",
      "Actual Label = frog vs. Predicted Label = horse\n",
      "Actual Label = frog vs. Predicted Label = truck\n",
      "Actual Label = automobile vs. Predicted Label = truck\n",
      "Actual Label = frog vs. Predicted Label = dog\n",
      "Actual Label = cat vs. Predicted Label = airplane\n",
      "Actual Label = automobile vs. Predicted Label = frog\n",
      "Actual Label = airplane vs. Predicted Label = dog\n",
      "Actual Label = truck vs. Predicted Label = frog\n",
      "Actual Label = dog vs. Predicted Label = airplane\n",
      "Actual Label = horse vs. Predicted Label = truck\n",
      "Actual Label = truck vs. Predicted Label = cat\n",
      "Actual Label = ship vs. Predicted Label = truck\n",
      "Actual Label = dog vs. Predicted Label = horse\n",
      "Actual Label = horse vs. Predicted Label = deer\n",
      "Actual Label = ship vs. Predicted Label = truck\n",
      "Actual Label = frog vs. Predicted Label = ship\n",
      "Actual Label = horse vs. Predicted Label = horse\n"
     ]
    }
   ],
   "source": [
    "#Model 3\n",
    "#Deep CNN  with Maxpooling and leakyrelu\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, advanced_activations\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 22\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model_1A.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model_3 = Sequential()\n",
    "\n",
    "model_3.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model_3.add(Conv2D(32, (3, 3)))\n",
    "model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "model_3.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model_3.add(Conv2D(64, (3, 3)))\n",
    "model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dense(512))\n",
    "model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(num_classes))\n",
    "model_3.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model_1A.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model_3.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Load label names to use in prediction results\n",
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "evaluation = model_3.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=x_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "predict_gen = model_3.predict_generator(datagen.flow(x_test, y_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=x_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "\n",
    "for predict_index, predicted_y in enumerate(predict_gen):\n",
    "    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n",
    "    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n",
    "                                                          predicted_label))\n",
    "    if predict_index == num_predictions:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assignment 1 Part 2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.concatenate((x_train, x_test), axis=0)\n",
    "y=np.concatenate((y_train,y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the entire dataset randomly\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train2c, X_test2c, y_train2c, y_test2c = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/25\n",
      "48000/48000 [==============================] - 165s - loss: 0.7787 - acc: 0.7323 - val_loss: 0.6853 - val_acc: 0.7658\n",
      "Epoch 2/25\n",
      "48000/48000 [==============================] - 167s - loss: 0.7592 - acc: 0.7380 - val_loss: 0.6701 - val_acc: 0.7693\n",
      "Epoch 3/25\n",
      "48000/48000 [==============================] - 168s - loss: 0.7444 - acc: 0.7458 - val_loss: 0.6883 - val_acc: 0.7614\n",
      "Epoch 4/25\n",
      "48000/48000 [==============================] - 167s - loss: 0.7333 - acc: 0.7468 - val_loss: 0.6604 - val_acc: 0.7725\n",
      "Epoch 5/25\n",
      "48000/48000 [==============================] - 165s - loss: 0.7190 - acc: 0.7516 - val_loss: 0.6565 - val_acc: 0.7754\n",
      "Epoch 6/25\n",
      "48000/48000 [==============================] - 167s - loss: 0.7067 - acc: 0.7571 - val_loss: 0.6478 - val_acc: 0.7775\n",
      "Epoch 7/25\n",
      "48000/48000 [==============================] - 167s - loss: 0.6977 - acc: 0.7604 - val_loss: 0.6510 - val_acc: 0.7777\n",
      "Epoch 8/25\n",
      "48000/48000 [==============================] - 170s - loss: 0.6868 - acc: 0.7639 - val_loss: 0.6564 - val_acc: 0.7727\n",
      "Epoch 9/25\n",
      "48000/48000 [==============================] - 168s - loss: 0.6786 - acc: 0.7659 - val_loss: 0.6367 - val_acc: 0.7819\n",
      "Epoch 10/25\n",
      "48000/48000 [==============================] - 169s - loss: 0.6741 - acc: 0.7674 - val_loss: 0.6453 - val_acc: 0.7797\n",
      "Epoch 11/25\n",
      "48000/48000 [==============================] - 169s - loss: 0.6588 - acc: 0.7730 - val_loss: 0.6572 - val_acc: 0.7754\n",
      "Epoch 12/25\n",
      "48000/48000 [==============================] - 169s - loss: 0.6603 - acc: 0.7734 - val_loss: 0.6268 - val_acc: 0.7831\n",
      "Epoch 13/25\n",
      "48000/48000 [==============================] - 169s - loss: 0.6488 - acc: 0.7759 - val_loss: 0.6187 - val_acc: 0.7832\n",
      "Epoch 14/25\n",
      "48000/48000 [==============================] - 168s - loss: 0.6429 - acc: 0.7792 - val_loss: 0.6385 - val_acc: 0.7792\n",
      "Epoch 15/25\n",
      "48000/48000 [==============================] - 168s - loss: 0.6336 - acc: 0.7797 - val_loss: 0.6177 - val_acc: 0.7859\n",
      "Epoch 16/25\n",
      "48000/48000 [==============================] - 167s - loss: 0.6279 - acc: 0.7850 - val_loss: 0.6126 - val_acc: 0.7847\n",
      "Epoch 17/25\n",
      "48000/48000 [==============================] - 167s - loss: 0.6200 - acc: 0.7858 - val_loss: 0.6113 - val_acc: 0.7876\n",
      "Epoch 18/25\n",
      "45280/48000 [===========================>..] - ETA: 8s - loss: 0.6142 - acc: 0.7895"
     ]
    }
   ],
   "source": [
    "model_3.fit(X_train2c, y_train2c, validation_data=(X_test2c, y_test2c), epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
