{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Simple CNN model for CIFAR-10\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import scipy\n",
    "from keras.metrics import categorical_accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import model_from_json\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sde1-bebf9d65efdfe8-478becfe92a9/.local/lib/python2.7/site-packages\r\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages (from h5py)\r\n",
      "Requirement already satisfied: six in /usr/local/src/bluemix_jupyter_bundle.v63/notebook/lib/python2.7/site-packages (from h5py)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default values of the following parameters #batch_size,nb_epochs,nb_classes,split,lr,optimizer,lossfunction,\n",
    "#activation,dropout\n",
    "#defaults and constants\n",
    "batch_size = 128\n",
    "nb_epochs = 20\n",
    "nb_classes = 10\n",
    "split = 0.2\n",
    "lrate = 0.001\n",
    "#optimizer = SGD\n",
    "loss = \"categorical_crossentropy\"\n",
    "activation = \"relu\"\n",
    "dropout = 0.2\n",
    "noofneurons=512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 5\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "#num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "def createmodel1(batchsize,epochs,num_classes,lrate,dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    epochs = epochs\n",
    "    lrate = lrate\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batchsize)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deep version of the simple CNN as shownabove.Introducing an additional round of convolutions with many more feature maps.\n",
    "#Using the same pattern of Convolutional, Dropout, Convolutional and Max Pooling layers.# Create the model\n",
    "def createmodel2(batchsize,epochs,num_classes,lrate,dropout):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    epochs = epochs #25\n",
    "    lrate = lrate\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model2.summary())\n",
    "    numpy.random.seed(seed)\n",
    "    model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batchsize)\n",
    "    # Final evaluation of the model\n",
    "    scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3\n",
    "#Deep CNN  with Maxpooling and leakyrelu\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, advanced_activations\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def createmodel3(batchsize,epochs,num_classes,lrate,dropout):\n",
    "    \n",
    "# The data, shuffled and split between train and test sets:\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    #print('x_train shape:', x_train.shape)\n",
    "    #print(x_train.shape[0], 'train samples')\n",
    "    #print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model_3 = Sequential()\n",
    "\n",
    "    model_3.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "    model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "    model_3.add(Conv2D(32, (3, 3)))\n",
    "    model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "    model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_3.add(Dropout(0.25))\n",
    "\n",
    "    model_3.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "    model_3.add(Conv2D(64, (3, 3)))\n",
    "    model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "    model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model_3.add(Dropout(0.25))\n",
    "\n",
    "    model_3.add(Flatten())\n",
    "    model_3.add(Dense(512))\n",
    "    model_3.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "    model_3.add(Dropout(0.5))\n",
    "    model_3.add(Dense(num_classes))\n",
    "    model_3.add(Activation('softmax'))\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model_1A.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model_3.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "    # Load label names to use in prediction results\n",
    "    label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n",
    "\n",
    "\n",
    "    keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "    datadir_base = os.path.expanduser(keras_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join('/tmp', '.keras')\n",
    "    label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "    with open(label_list_path, mode='rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "\n",
    "# Evaluate model with test data set and share sample prediction results\n",
    "    evaluation = model_3.evaluate_generator(datagen.flow(x_test, y_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=x_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "    print('Model Accuracy = %.2f' % (evaluation[1]))\n",
    "\n",
    "    predict_gen = model_3.predict_generator(datagen.flow(x_test, y_test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=False),\n",
    "                                      steps=x_test.shape[0] // batch_size,\n",
    "                                      workers=4)\n",
    "\n",
    "    for predict_index, predicted_y in enumerate(predict_gen):\n",
    "        actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n",
    "        predicted_label = labels['label_names'][np.argmax(predicted_y)]\n",
    "        print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n",
    "                                                          predicted_label))\n",
    "        if predict_index == num_predictions:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batchsize</th>\n",
       "      <th>epochs</th>\n",
       "      <th>num_classes</th>\n",
       "      <th>lrate</th>\n",
       "      <th>dropout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batchsize  epochs  num_classes  lrate  dropout\n",
       "0         32      25           10  0.010     0.20\n",
       "1         64      25           10  0.010     0.20\n",
       "2         32      20           10  0.012     0.25\n",
       "3         32      25           10  0.010     0.20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# @hidden_cell\n",
    "# This function accesses a file in your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "def get_object_storage_file_with_credentials_f2b31d3aa8f74da9a868a4102976b52e(container, filename):\n",
    "    \"\"\"This functions returns a StringIO object containing\n",
    "    the file content from Bluemix Object Storage.\"\"\"\n",
    "\n",
    "    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n",
    "    data = {'auth': {'identity': {'methods': ['password'],\n",
    "            'password': {'user': {'name': 'member_eaa7a3ccbee220b104765fd6ecbda6648009a26f','domain': {'id': 'a76f4599636a4d9fb2a2209d58f8cb53'},\n",
    "            'password': 'f=O00,1/BLCkVq-Q'}}}}}\n",
    "    headers1 = {'Content-Type': 'application/json'}\n",
    "    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n",
    "    resp1_body = resp1.json()\n",
    "    for e1 in resp1_body['token']['catalog']:\n",
    "        if(e1['type']=='object-store'):\n",
    "            for e2 in e1['endpoints']:\n",
    "                        if(e2['interface']=='public'and e2['region']=='dallas'):\n",
    "                            url2 = ''.join([e2['url'],'/', container, '/', filename])\n",
    "    s_subject_token = resp1.headers['x-subject-token']\n",
    "    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n",
    "    resp2 = requests.get(url=url2, headers=headers2)\n",
    "    return StringIO(resp2.text)\n",
    "\n",
    "df = pd.read_csv(get_object_storage_file_with_credentials_f2b31d3aa8f74da9a868a4102976b52e('DefaultProjectlekhrajanijhuskyneuedu', 'Inputs.csv'))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Model 1 on the dataset(Activation= Relu&Softmax, Optimizer=SGD())Basic CNN\n",
    "def createmodels(df):\n",
    "    for index, row in df.iterrows():\n",
    "        createmodel1(row['batchsize'].astype(int),row['epochs'].astype(int),row['num_classes'].astype(int),row['lrate'],row['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createmodels(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Model 2 on dataset(Activation= Relu&Softmax, Optimizer=SGD())Deep CNN\n",
    "def createmodels(df):\n",
    "    for index, row in df.iterrows():\n",
    "        createmodel2(row['batchsize'].astype(int),row['epochs'].astype(int),row['num_classes'].astype(int),row['lrate'],row['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createmodels(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Model 3 on dataset(Activation=Relu&Softmax, Optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createmodels(df):\n",
    "    for index, row in df.iterrows():\n",
    "        createmodel3(row['batchsize'],row['epochs'],row['num_classes'],row['lrate'],row['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createmodels(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assignment 1 Part 2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.concatenate((X_train, X_test), axis=0)\n",
    "y=np.concatenate((y_train,y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state=5\n",
    "X_train2c, X_test2c, y_train2c, y_test2c = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating labels(code taken from the base code)\n",
    "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\n",
    "datadir_base = os.path.expanduser(keras_dir)\n",
    "if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join('/tmp', '.keras')\n",
    "label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "\n",
    "with open(label_list_path, mode='rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane 108000.0\n",
      "automobile 12000.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import itemfreq\n",
    "a= itemfreq(y_test2c)\n",
    "for i in range (0,len(a)):\n",
    "    print(labels['label_names'][i],a[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 10793, 1.0: 1207})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_test2c[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.metrics import categorical_accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 16, 16)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 128, 8, 8)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 128, 8, 8)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 128, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,915,114\n",
      "Trainable params: 2,915,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 231s - loss: 1.9634 - acc: 0.2698 - val_loss: 1.7156 - val_acc: 0.3665\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 203s - loss: 1.5272 - acc: 0.4412 - val_loss: 1.3940 - val_acc: 0.4968\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 209s - loss: 1.3381 - acc: 0.5149 - val_loss: 1.2046 - val_acc: 0.5680\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 208s - loss: 1.2034 - acc: 0.5666 - val_loss: 1.1285 - val_acc: 0.5962\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 202s - loss: 1.0939 - acc: 0.6061 - val_loss: 1.0060 - val_acc: 0.6435\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 208s - loss: 0.9997 - acc: 0.6424 - val_loss: 0.9525 - val_acc: 0.6573\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 229s - loss: 0.9254 - acc: 0.6711 - val_loss: 0.9312 - val_acc: 0.6733\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.8660 - acc: 0.6937 - val_loss: 0.8427 - val_acc: 0.7010\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.8139 - acc: 0.7118 - val_loss: 0.8077 - val_acc: 0.7182\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 199s - loss: 0.7645 - acc: 0.7287 - val_loss: 0.8017 - val_acc: 0.7216\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.7295 - acc: 0.7416 - val_loss: 0.7495 - val_acc: 0.7394\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 203s - loss: 0.6880 - acc: 0.7547 - val_loss: 0.7282 - val_acc: 0.7472\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.6605 - acc: 0.7646 - val_loss: 0.7031 - val_acc: 0.7559\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.6278 - acc: 0.7766 - val_loss: 0.6975 - val_acc: 0.7559\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.6058 - acc: 0.7854 - val_loss: 0.6964 - val_acc: 0.7595\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.5797 - acc: 0.7935 - val_loss: 0.6729 - val_acc: 0.7663\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 199s - loss: 0.5682 - acc: 0.7981 - val_loss: 0.6625 - val_acc: 0.7728\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 199s - loss: 0.5382 - acc: 0.8093 - val_loss: 0.6637 - val_acc: 0.7700\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 201s - loss: 0.5243 - acc: 0.8143 - val_loss: 0.6606 - val_acc: 0.7743\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.5055 - acc: 0.8203 - val_loss: 0.6382 - val_acc: 0.7776\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 199s - loss: 0.4885 - acc: 0.8255 - val_loss: 0.6312 - val_acc: 0.7833\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.4690 - acc: 0.8332 - val_loss: 0.6505 - val_acc: 0.7755\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.4544 - acc: 0.8380 - val_loss: 0.6620 - val_acc: 0.7784\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 200s - loss: 0.4435 - acc: 0.8415 - val_loss: 0.6349 - val_acc: 0.7881\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 199s - loss: 0.4235 - acc: 0.8489 - val_loss: 0.6262 - val_acc: 0.7902\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "numpy.random.seed(seed)\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)\n",
    "model2.save('model2.h5')\n",
    "    # Final evaluation of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.75%\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test2c, y_test2c, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Models import model1,model2,model3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deep version of the simple CNN as shownabove.Introducing an additional round of convolutions with many more feature maps.\n",
    "#Using the same pattern of Convolutional, Dropout, Convolutional and Max Pooling layers.# Create the model\n",
    "def createAndSaveModel(batchsize,epochs,num_classes,lrate,dropout,modelname,weights):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model2.add(Dropout(dropout))\n",
    "    model2.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    epochs = epochs #25\n",
    "    lrate = lrate\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    print(model2.summary())\n",
    "    numpy.random.seed(seed)\n",
    "    model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batchsize)\n",
    "    # Final evaluation of the model\n",
    "    scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    #save model to json format\n",
    "    modeljson = model2.to_json()\n",
    "    open(modelname, 'w').write(modeljson)\n",
    "\n",
    "    #Save the weights learnt\n",
    "    model2.save_weights(weights,overwrite=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createAndSaveModel(*model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(\"model2_cifar10.json.json\", \"r\")\n",
    "model2loaded = json_file.read()\n",
    "son_file.close()\n",
    "modeltobetested = model_from_json(model2loaded)\n",
    "modeltobetested.load_weights(\"model2_cifar10_weights.h5\")\n",
    "# Evaluate model on test data\n",
    "predictedvalues = loaded_model.predict_classes(X_test2c)\n",
    "print(predictedvalues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Accuracy when trained using 12,000 images instead of running the best model that has been built earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating 12000 random traing and testing data from orignal data\n",
    "x_total =np.concatenate((X_train,X_test),axis=0)\n",
    "y_total =np.concatenate((y_train,y_test),axis=0)\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "random = np.random.randint(0,60000,size=12000)\n",
    "x_train_new = x_total[random]\n",
    "y_train_new = y_total[random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane 108000.0\n",
      "automobile 12000.0\n"
     ]
    }
   ],
   "source": [
    "#summary of the images \n",
    "a= itemfreq(y_train_new)\n",
    "for i in range (0,len(a)):\n",
    "    print(labels['label_names'][i],a[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 64, 16, 16)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 128, 8, 8)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 128, 8, 8)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 128, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,915,114\n",
      "Trainable params: 2,915,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 9600 samples, validate on 2400 samples\n",
      "Epoch 1/25\n",
      "9600/9600 [==============================] - 39s - loss: 2.2264 - acc: 0.1554 - val_loss: 2.1433 - val_acc: 0.2096\n",
      "Epoch 2/25\n",
      "9600/9600 [==============================] - 38s - loss: 2.0302 - acc: 0.2550 - val_loss: 1.9487 - val_acc: 0.3000\n",
      "Epoch 3/25\n",
      "9600/9600 [==============================] - 39s - loss: 1.9375 - acc: 0.2922 - val_loss: 1.8640 - val_acc: 0.3438\n",
      "Epoch 4/25\n",
      "9600/9600 [==============================] - 38s - loss: 1.8120 - acc: 0.3398 - val_loss: 1.7047 - val_acc: 0.3796\n",
      "Epoch 5/25\n",
      "9600/9600 [==============================] - 38s - loss: 1.6666 - acc: 0.3890 - val_loss: 1.6374 - val_acc: 0.4258\n",
      "Epoch 6/25\n",
      "9600/9600 [==============================] - 38s - loss: 1.5877 - acc: 0.4238 - val_loss: 1.5269 - val_acc: 0.4533\n",
      "Epoch 7/25\n",
      "9600/9600 [==============================] - 38s - loss: 1.5227 - acc: 0.4459 - val_loss: 1.4800 - val_acc: 0.4650\n",
      "Epoch 8/25\n",
      "9600/9600 [==============================] - 39s - loss: 1.4471 - acc: 0.4730 - val_loss: 1.3931 - val_acc: 0.5083\n",
      "Epoch 9/25\n",
      "9600/9600 [==============================] - 38s - loss: 1.3932 - acc: 0.4910 - val_loss: 1.4476 - val_acc: 0.4888\n",
      "Epoch 10/25\n",
      "9600/9600 [==============================] - 38s - loss: 1.3346 - acc: 0.5184 - val_loss: 1.3477 - val_acc: 0.5121\n",
      "Epoch 11/25\n",
      "9600/9600 [==============================] - 39s - loss: 1.2865 - acc: 0.5376 - val_loss: 1.3095 - val_acc: 0.5246\n",
      "Epoch 12/25\n",
      "9600/9600 [==============================] - 39s - loss: 1.2377 - acc: 0.5541 - val_loss: 1.2559 - val_acc: 0.5467\n",
      "Epoch 13/25\n",
      "9600/9600 [==============================] - 39s - loss: 1.1815 - acc: 0.5741 - val_loss: 1.2387 - val_acc: 0.5587\n",
      "Epoch 14/25\n",
      "9600/9600 [==============================] - 39s - loss: 1.1443 - acc: 0.5879 - val_loss: 1.2312 - val_acc: 0.5617\n",
      "Epoch 15/25\n",
      "9600/9600 [==============================] - 39s - loss: 1.0990 - acc: 0.6055 - val_loss: 1.1845 - val_acc: 0.5792\n",
      "Epoch 16/25\n",
      "9600/9600 [==============================] - 38s - loss: 1.0417 - acc: 0.6259 - val_loss: 1.2044 - val_acc: 0.5767\n",
      "Epoch 17/25\n",
      "9600/9600 [==============================] - 39s - loss: 0.9847 - acc: 0.6442 - val_loss: 1.1759 - val_acc: 0.5825\n",
      "Epoch 18/25\n",
      "9600/9600 [==============================] - 39s - loss: 0.9479 - acc: 0.6628 - val_loss: 1.1900 - val_acc: 0.5767\n",
      "Epoch 19/25\n",
      "9600/9600 [==============================] - 39s - loss: 0.9045 - acc: 0.6763 - val_loss: 1.1466 - val_acc: 0.6075\n",
      "Epoch 20/25\n",
      "9600/9600 [==============================] - 38s - loss: 0.8452 - acc: 0.6968 - val_loss: 1.0990 - val_acc: 0.6212\n",
      "Epoch 21/25\n",
      "9600/9600 [==============================] - 38s - loss: 0.8020 - acc: 0.7173 - val_loss: 1.1212 - val_acc: 0.6167\n",
      "Epoch 22/25\n",
      "9600/9600 [==============================] - 39s - loss: 0.7468 - acc: 0.7315 - val_loss: 1.1244 - val_acc: 0.6200\n",
      "Epoch 23/25\n",
      "9600/9600 [==============================] - 38s - loss: 0.6987 - acc: 0.7520 - val_loss: 1.1132 - val_acc: 0.6242\n",
      "Epoch 24/25\n",
      "9600/9600 [==============================] - 38s - loss: 0.6623 - acc: 0.7681 - val_loss: 1.1038 - val_acc: 0.6254\n",
      "Epoch 25/25\n",
      "9600/9600 [==============================] - 39s - loss: 0.6196 - acc: 0.7803 - val_loss: 1.1200 - val_acc: 0.6346\n"
     ]
    }
   ],
   "source": [
    "modelnew = Sequential()\n",
    "modelnew.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "modelnew.add(Dropout(0.2))\n",
    "modelnew.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "modelnew.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelnew.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "modelnew.add(Dropout(0.2))\n",
    "modelnew.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "modelnew.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelnew.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "modelnew.add(Dropout(0.2))\n",
    "modelnew.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "modelnew.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "modelnew.add(Flatten())\n",
    "modelnew.add(Dropout(0.2))\n",
    "modelnew.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "modelnew.add(Dropout(0.2))\n",
    "modelnew.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "modelnew.add(Dropout(0.2))\n",
    "modelnew.add(Dense(10, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "modelnew.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(modelnew.summary())\n",
    "numpy.random.seed(seed)\n",
    "modelnew.fit(x_train_new, y_train_new,validation_split=0.2, epochs=epochs, batch_size=64)\n",
    "modelnew.save('modelnew.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
